{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifies the location of the dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './data'\n",
    "LABELS = ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stop word used is in English, because the data used is in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = set(stopwords.words('english'))\n",
    "stop_word.add('said')\n",
    "stop_word.add('mr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the data contained in the data folder. Then write the data back to the data.txt file. The data in the data.txt file will be labeled according to the location of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set():\n",
    "    with open('data.txt', 'w', encoding='utf8') as outfile:\n",
    "        for label in LABELS:\n",
    "            dir = '%s/%s' % (BASE_DIR, label)\n",
    "            for filename in os.listdir(dir):\n",
    "                fullfilename = '%s/%s' % (dir, filename)\n",
    "                with open(fullfilename, 'rb') as file:\n",
    "                    text = file.read().decode(errors='replace').replace('\\n', '')\n",
    "                    outfile.write('%s\\t%s\\t%s\\n' % (label, filename, text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the necessary data set-up, by taking the label data and news text, which are then stored in the docs variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_doc():\n",
    "    docs = [] #[(label, text)]\n",
    "    with open('data.txt', 'r', encoding='utf8') as datafile:\n",
    "        for row in datafile:\n",
    "            parts = row.split('\\t')\n",
    "            doc = (parts[0], parts[2].strip())\n",
    "            \n",
    "            docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete unnecessary words and change all letters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if not t in stop_word]\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the frequency of each word that appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_dist(docs):\n",
    "    tokens = defaultdict(list)\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_label = doc[0]\n",
    "        doc_text = clean_text(doc[1])\n",
    "        \n",
    "        doc_tokens = get_tokens(doc_text)\n",
    "        tokens[doc_label].extend(doc_tokens)\n",
    "    \n",
    "    for category_label, category_tokens in tokens.items():\n",
    "        print(category_label)\n",
    "        fd = FreqDist(category_tokens)\n",
    "        print(fd.most_common(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data train and data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(docs):\n",
    "    \n",
    "    random.shuffle(docs)\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    X_test = []\n",
    "    y_test= []\n",
    "    \n",
    "    pivot = int(.80 * len(docs))\n",
    "    \n",
    "    for i in range(0, pivot):\n",
    "        X_train.append(docs[i][1])\n",
    "        y_train.append(docs[i][0])\n",
    "        \n",
    "    for i in range(pivot, len(docs)):\n",
    "        X_test.append(docs[i][1])\n",
    "        y_test.append(docs[i][0])\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results of the model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(title, classifier, vectorizer, X_test, y_test):\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "    \n",
    "    precision = metrics.precision_score(y_test, y_pred, average='micro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print('%s\\t%f\\t%f\\t%f\\n' % (title, precision, recall, f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(docs):\n",
    "    X_train, X_test, y_train, y_test = get_split(docs)\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words='english',\n",
    "                                 ngram_range=(1, 3),\n",
    "                                 min_df=3, analyzer='word')\n",
    "    \n",
    "    dtm = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    naive_bayes_classifier = MultinomialNB().fit(dtm, y_train)\n",
    "   \n",
    "    # evaluate_classifier(\"Naive Bayes\\tTRAINT\\t\", naive_bayes_classifier, vectorizer, X_train, y_train)\n",
    "    # evaluate_classifier(\"Naive Bayes\\tTEST\\t\", naive_bayes_classifier, vectorizer, X_test, y_test)\n",
    "    \n",
    "    clf_filename = 'naive_bayes_classifire.pkl'\n",
    "    pickle.dump(naive_bayes_classifier, open(clf_filename, 'wb'))\n",
    "    \n",
    "    vec_filename = 'count_vectorize.pkl'\n",
    "    pickle.dump(vectorizer, open(vec_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ =='__main__':\n",
    "    # Create dataset \n",
    "    # create_data_set()\n",
    "    \n",
    "    # Setup data set\n",
    "    docs = setup_doc()\n",
    "    \n",
    "    # Cek frequency in data set\n",
    "    # frequency_dist(docs)\n",
    "    \n",
    "    # Export model to .pkl\n",
    "    train_classifier(docs)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
